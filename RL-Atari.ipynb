{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(obs):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(obs)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obs(obs):\n",
    "    obs = obs.max(axis=-1,keepdims=1)\n",
    "    obs = obs.reshape((210,160))\n",
    "    obs = misc.imresize(obs, (110,84))\n",
    "    lives = obs[2:9,52:59]\n",
    "    obs = obs[18:102,:,]\n",
    "    obs = np.expand_dims(obs,2)\n",
    "    obs = obs.astype(\"float32\") / 255\n",
    "    obs[obs > 0] = 1\n",
    "    return [obs,lives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "init = env.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Welcome to AI Breakout!\")\n",
    "    print(\"Starting Test Game...\\n\")\n",
    "\n",
    "    \n",
    "    game = True\n",
    "    LIVES = []\n",
    "    while game:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        LIVES.append(lives)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            LIVES = np.unique(np.array(LIVES), axis=0)\n",
    "            \n",
    "            for i in LIVES:\n",
    "                plot_state(i.reshape(7,7))\n",
    "            game = False\n",
    "            observation = env.reset()\n",
    "                             \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "init = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = process_obs(init)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(I.reshape(84,84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(84,84,4))\n",
    "x = Conv2D(16, (8,8), strides=4, activation=\"relu\", input_shape=(84,84,4))(inp)\n",
    "x = Conv2D(32, (4,4), strides=2, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,  activation=\"relu\")(x)\n",
    "output = Dense(env.action_space.n)(x)\n",
    "\n",
    "model = Model(inp, output)\n",
    "callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.01, patience=2, mode = min, verbose = 1)]\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"mae\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFE = {}\n",
    "LIFE[\"1\"] = LIVES[0]\n",
    "LIFE[\"2\"] = LIVES[3]\n",
    "LIFE[\"3\"] = LIVES[2]\n",
    "LIFE[\"4\"] = LIVES[1]\n",
    "LIFE[\"5\"] = LIVES[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in LIFE.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_episode_history = {\"state\" : np.zeros(shape=(400000,84,84,4), dtype=\"float32\"),\\\n",
    "                        \"reward\" : np.zeros(shape=(400000,), dtype=\"float32\"),\\\n",
    "                        \"action\": np.zeros(shape=(400000,), dtype=\"float32\"), \\\n",
    "                        \"Q\": np.zeros(shape=(400000,4), dtype=\"float32\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "observation, lives = process_obs(observation)\n",
    "\n",
    "episodes = 150000\n",
    "episode_scores = np.zeros(shape=(150001))\n",
    "total_frames = 0\n",
    "sum_total_frames = 0\n",
    "count = 0\n",
    "epsilon = 1.0\n",
    "gamma = 0.99\n",
    "done = False\n",
    "\n",
    "for i in tqdm(range(1, episodes+1)):\n",
    "    \n",
    "    if (i % 1000 == 0) or (i == 1):\n",
    "        print(\"Welcome to AI Breakout!\")\n",
    "        print(\"Starting Game {} of {}...\\n\".format(i, episodes))\n",
    "    \n",
    "    sum = 0\n",
    "    episode_frame = 1\n",
    "    game = True\n",
    "    LIVES = []\n",
    "    observation_batch = []\n",
    "    reward_batch = []\n",
    "    episode_history = {\"state\" : np.array([]), \"reward\" : np.array([]), \"action\": np.array([]), \"Q\": np.array([])}\n",
    "\n",
    "    while game: \n",
    "        if (episode_frame == 1):\n",
    "            action = 1              \n",
    "        else:\n",
    "            if (episode_frame - 1) % 4 == 0:\n",
    "                rand = np.random.uniform(0,1)\n",
    "                if  rand < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    observation = np.expand_dims(episode_history[\"state\"][-1],0)\n",
    "                    action = np.argmax(model.predict(observation))\n",
    "                \n",
    "                if len(episode_history[\"state\"]) > 2:\n",
    "                    if np.array_equal(episode_history[\"state\"][-1], episode_history[\"state\"][-2]):\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                observation_batch = []\n",
    "                reward_batch = []\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        \n",
    "        if len(LIVES) == 0:\n",
    "            LIVES.append(lives)\n",
    "            loss = 0\n",
    "        else:\n",
    "            if np.array_equal(LIVES[-1], lives):\n",
    "                loss = 0\n",
    "            else:\n",
    "                LIVES.append(lives)\n",
    "                loss = -10\n",
    "                \n",
    "        observation_batch.append(observation)\n",
    "        reward_batch.append(10*reward+loss)        \n",
    "        \n",
    "        if (episode_frame % 4 == 0):\n",
    "            if episode_frame == 4:\n",
    "                episode_history[\"state\"] = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "                episode_history[\"state\"] = np.expand_dims(episode_history[\"state\"],0)\n",
    "\n",
    "            else:\n",
    "                obs = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "                obs = np.expand_dims(obs,0)\n",
    "                episode_history[\"state\"] = np.concatenate([episode_history[\"state\"], obs], axis=0)\n",
    "\n",
    "            episode_history[\"reward\"] = np.concatenate([episode_history[\"reward\"], np.array([np.sum(reward_batch)])], axis = 0)  \n",
    "            episode_history[\"action\"] = np.concatenate([episode_history[\"action\"], np.array([action])], axis=0)\n",
    "         \n",
    "        sum += reward\n",
    "        episode_frame += 1\n",
    "        \n",
    "        if done:\n",
    "            total_frames += episode_history[\"state\"].shape[0]\n",
    "            sum_total_frames += episode_history[\"state\"].shape[0]\n",
    "            \n",
    "            for j,k in enumerate(episode_history[\"reward\"][::-1]):\n",
    "                \n",
    "                if j == 0:\n",
    "                    val = np.array([[0, 0, 0, 0]], dtype=\"float32\")\n",
    "                    val[0,int(episode_history[\"action\"][-1])] = k\n",
    "                    Q = val\n",
    "                else:\n",
    "                    val = np.array([[0, 0, 0, 0]], dtype=\"float32\")\n",
    "                    val[0,int(episode_history[\"action\"][-1-j])] = k + gamma*np.sum(Q[-1])\n",
    "                    Q = np.concatenate([Q,val], axis = 0)\n",
    "\n",
    "            Q = Q[::-1]\n",
    "            episode_history[\"Q\"] = Q\n",
    "            \n",
    "            try:\n",
    "                full_episode_history[\"state\"][total_frames - episode_history[\"state\"].shape[0]: total_frames,:,:,:] = episode_history[\"state\"]\n",
    "                full_episode_history[\"reward\"][total_frames - episode_history[\"state\"].shape[0]: total_frames] = episode_history[\"reward\"]\n",
    "                full_episode_history[\"action\"][total_frames - episode_history[\"state\"].shape[0]: total_frames] = episode_history[\"action\"]\n",
    "                full_episode_history[\"Q\"][total_frames - episode_history[\"state\"].shape[0]: total_frames,:] = episode_history[\"Q\"]\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                with open(\"RL-atari-batch-data-{}.pkl\".format(count), \"wb\") as f:\n",
    "                    pickle.dump(full_episode_history, f)\n",
    "        \n",
    "                total_frames = episode_history[\"state\"].shape[0]\n",
    "                full_episode_history = {\"state\" : np.zeros(shape=(400000,84,84,4), dtype=\"float32\"),\\\n",
    "                                        \"reward\" : np.zeros(shape=(400000,), dtype=\"float32\"),\\\n",
    "                                        \"action\": np.zeros(shape=(400000,), dtype=\"float32\"), \\\n",
    "                                        \"Q\": np.zeros(shape=(400000,4), dtype=\"float32\")}\n",
    "                \n",
    "                full_episode_history[\"state\"][total_frames - episode_history[\"state\"].shape[0]: total_frames,:,:,:] = episode_history[\"state\"]\n",
    "                full_episode_history[\"reward\"][total_frames - episode_history[\"state\"].shape[0]: total_frames] = episode_history[\"reward\"]\n",
    "                full_episode_history[\"action\"][total_frames - episode_history[\"state\"].shape[0]: total_frames] = episode_history[\"action\"]\n",
    "                full_episode_history[\"Q\"][total_frames - episode_history[\"state\"].shape[0]: total_frames,:] = episode_history[\"Q\"]\n",
    "\n",
    "\n",
    "            #print(\"Training the model from a random batch...\")\n",
    "            \n",
    "            try:\n",
    "                mask = np.random.choice(range(total_frames), size=128)\n",
    "                inp = full_episode_history[\"state\"][mask]\n",
    "                target = full_episode_history[\"Q\"][mask]\n",
    "                model.fit(inp, target, epochs = 5, batch_size=8, verbose=0)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if (i % 1000 == 0) or (i == 1): \n",
    "                print(\"Average game score: {}\".format(np.mean(episode_scores[i-1000:i])))\n",
    "                print(\"Total frames so far: {}\".format(sum_total_frames))\n",
    "                print(\"Epsilon: {}\\n\".format(epsilon))\n",
    "             \n",
    "            if (i % 1000 == 0):\n",
    "                plt.figure(figsize=(17,11))\n",
    "                plt.plot(episode_scores[i-1000:i])\n",
    "                plt.show()\n",
    "            \n",
    "            observation = env.reset()\n",
    "            observation, lives = process_obs(observation)\n",
    "            episode_scores[i] = sum\n",
    "            \n",
    "            m = -7e-7\n",
    "            epsilon = round(m*sum_total_frames +1,5)\n",
    "            if epsilon < 0.1:\n",
    "                epsilon = 0.1\n",
    "            game = False\n",
    "            episode_frame = 0\n",
    "                             \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_episode_history[\"state\"].shape)\n",
    "print(episode_history[\"reward\"].shape)\n",
    "print(episode_history[\"action\"].shape)\n",
    "print(episode_history[\"Q\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"reward\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"Q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(episode_history[\"state\"][10],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(episode_scores).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,11))\n",
    "plt.plot(episode_scores[::10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"breakout-atari-rl.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mod = load_model(\"breakout-atari-rl.h5\")\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "observation, lives = process_obs(observation)\n",
    "\n",
    "episodes = 10\n",
    "done = False\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    print(\"Welcome to AI Breakout!\")\n",
    "    print(\"Starting Game {} of {}...\".format(i, episodes))\n",
    "    \n",
    "    sum = 0\n",
    "    frame = 1\n",
    "    game = True\n",
    "    observation_batch = []\n",
    "    while game:\n",
    "\n",
    "        if (frame == 1):\n",
    "            action = 1              \n",
    "        else:\n",
    "            if (frame - 1) % 4 == 0:\n",
    "                action = np.argmax(mod.predict(states))\n",
    "\n",
    "                if np.array_equal(states[0][:84,:84,0], states[0][:84,:84,3]):\n",
    "                    action = 1\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                observation_batch = []                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        observation_batch.append(observation)\n",
    "        \n",
    "        if (frame % 4 == 0):\n",
    "            states = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "            states = np.expand_dims(states,0)\n",
    "\n",
    "        sum += reward\n",
    "        frame += 1\n",
    "        time.sleep(0.03)\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            print(\"Total game score was: {}\\n\".format(int(sum)))\n",
    "            observation = env.reset()\n",
    "            observation, lives = process_obs(observation)\n",
    "            game = False\n",
    "env.close()        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
