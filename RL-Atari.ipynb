{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from scipy import ndimage, misc\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(obs):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(obs)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obs(obs):\n",
    "    obs = obs.max(axis=-1,keepdims=1)\n",
    "    obs = obs.reshape((210,160))\n",
    "    obs = misc.imresize(obs, (110,84))\n",
    "    lives = obs[2:9,52:59]\n",
    "    obs = obs[18:102,:,]\n",
    "    obs = np.expand_dims(obs,2)\n",
    "    obs = obs.astype(np.uint8) / 255\n",
    "    obs[obs > 0] = 1\n",
    "    return [obs,lives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "init = env.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Welcome to AI Breakout!\")\n",
    "    print(\"Starting Test Game...\\n\")\n",
    "\n",
    "    \n",
    "    game = True\n",
    "    LIVES = []\n",
    "    while game:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        LIVES.append(lives)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            LIVES = np.unique(np.array(LIVES), axis=0)\n",
    "            \n",
    "            for i in LIVES:\n",
    "                plot_state(i.reshape(7,7))\n",
    "            game = False\n",
    "            observation = env.reset()\n",
    "                             \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "init = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = process_obs(init)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(I.reshape(84,84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(84,84,4))\n",
    "x = Conv2D(16, (8,8), strides=4, activation=\"relu\", input_shape=(84,84,4))(inp)\n",
    "x = Conv2D(32, (4,4), strides=2, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,  activation=\"relu\")(x)\n",
    "output = Dense(env.action_space.n)(x)\n",
    "\n",
    "model = Model(inp, output)\n",
    "callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.01, patience=2, mode = min, verbose = 1)]\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"mae\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFE = {}\n",
    "LIFE[\"1\"] = LIVES[0]\n",
    "LIFE[\"2\"] = LIVES[3]\n",
    "LIFE[\"3\"] = LIVES[2]\n",
    "LIFE[\"4\"] = LIVES[1]\n",
    "LIFE[\"5\"] = LIVES[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in LIFE.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_episode_history = {\"state\" : np.array([]), \"reward\" : np.array([]), \"action\": np.array([]), \"Q\": np.array([])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "observation, lives = process_obs(observation)\n",
    "\n",
    "episodes = 500\n",
    "episode_scores = []\n",
    "epsilon = 0.5\n",
    "gamma = 0.99\n",
    "done = False\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    print(\"Welcome to AI Breakout!\")\n",
    "    print(\"Starting Game {} of {}...\\n\".format(i, episodes))\n",
    "    \n",
    "    sum = 0\n",
    "    frame = 1\n",
    "    game = True\n",
    "    LIVES = []\n",
    "    observation_batch = []\n",
    "    reward_batch = []\n",
    "    episode_history = {\"state\" : np.array([]), \"reward\" : np.array([]), \"action\": np.array([]), \"Q\": np.array([])}\n",
    "\n",
    "    \n",
    "    \n",
    "    while game: \n",
    "        \n",
    "        if (frame == 1):\n",
    "            action = 1              \n",
    "        else:\n",
    "            if (frame - 1) % 4 == 0:\n",
    "                rand = np.random.uniform(0,1)\n",
    "                if  rand < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    observation = np.expand_dims(episode_history[\"state\"][-1],0)\n",
    "                    action = np.argmax(model.predict(observation))\n",
    "                \n",
    "                if len(episode_history[\"state\"]) > 2:\n",
    "                    if np.array_equal(episode_history[\"state\"][-1], episode_history[\"state\"][-2]):\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                observation_batch = []\n",
    "                reward_batch = []\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        \n",
    "        if len(LIVES) == 0:\n",
    "            LIVES.append(lives)\n",
    "            loss = 0\n",
    "        else:\n",
    "            if np.array_equal(LIVES[-1], lives):\n",
    "                loss = 0\n",
    "            else:\n",
    "                LIVES.append(lives)\n",
    "                loss = -10\n",
    "                \n",
    "        observation_batch.append(observation)\n",
    "        reward_batch.append(10*reward+loss)        \n",
    "        \n",
    "        if (frame % 4 == 0):\n",
    "            if frame == 4:\n",
    "                episode_history[\"state\"] = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "                episode_history[\"state\"] = np.expand_dims(episode_history[\"state\"],0)\n",
    "\n",
    "            else:\n",
    "                obs = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "                obs = np.expand_dims(obs,0)\n",
    "                episode_history[\"state\"] = np.concatenate([episode_history[\"state\"], obs], axis=0)\n",
    "\n",
    "            episode_history[\"reward\"] = np.concatenate([episode_history[\"reward\"], np.array([np.sum(reward_batch)])], axis = 0)  \n",
    "            episode_history[\"action\"] = np.concatenate([episode_history[\"action\"], np.array([action])], axis=0)\n",
    "         \n",
    "        sum += reward\n",
    "        frame += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "\n",
    "            for j,k in enumerate(episode_history[\"reward\"][::-1]):\n",
    "                \n",
    "                if j == 0:\n",
    "                    val = np.array([[0, 0, 0, 0]], dtype=\"float32\")\n",
    "                    val[0,int(episode_history[\"action\"][-1])] = k\n",
    "                    Q = val\n",
    "                else:\n",
    "                    val = np.array([[0, 0, 0, 0]], dtype=\"float32\")\n",
    "                    val[0,int(episode_history[\"action\"][-1-j])] = k + gamma*np.sum(Q[-1])\n",
    "                    Q = np.concatenate([Q,val], axis = 0)\n",
    "                    \n",
    "            Q = Q[::-1]\n",
    "            episode_history[\"Q\"] = Q\n",
    "                        \n",
    "            if i == 1:\n",
    "                full_episode_history[\"state\"] = episode_history[\"state\"]\n",
    "                full_episode_history[\"Q\"] = episode_history[\"Q\"]\n",
    "            else:\n",
    "                full_episode_history[\"state\"] = np.concatenate([full_episode_history[\"state\"], episode_history[\"state\"]], axis = 0)\n",
    "                full_episode_history[\"Q\"] = np.concatenate([full_episode_history[\"Q\"], episode_history[\"Q\"]], axis = 0)\n",
    "\n",
    "            full_episode_history[\"reward\"] = np.concatenate([full_episode_history[\"reward\"], episode_history[\"reward\"]], axis = 0)\n",
    "            full_episode_history[\"action\"] = np.concatenate([full_episode_history[\"action\"], episode_history[\"action\"]], axis = 0)\n",
    "            \n",
    "            \n",
    "            print(\"Training the model from a random batch...\")\n",
    "            \n",
    "            rand = np.random.choice([i for i in range(full_episode_history[\"action\"].shape[0])], size=32)\n",
    "            inp = full_episode_history[\"state\"][rand]\n",
    "            target = full_episode_history[\"Q\"][rand]\n",
    "            model.fit(inp, target, epochs = 3, batch_size=8, verbose=1)\n",
    "            print(\"Total game score was: {}\".format(int(sum)))\n",
    "            print(\"Epsilon: {}\\n\".format(epsilon))\n",
    "            \n",
    "            observation = env.reset()\n",
    "            observation, lives = process_obs(observation)\n",
    "            episode_scores.append(int(sum))\n",
    "            epsilon *= 0.999\n",
    "            if epsilon < 0.1:\n",
    "                epsilon = 0.1\n",
    "            game = False\n",
    "            frame = 0\n",
    " \n",
    "                             \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_episode_history[\"state\"].shape)\n",
    "print(episode_history[\"reward\"].shape)\n",
    "print(episode_history[\"action\"].shape)\n",
    "print(episode_history[\"Q\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[-2.1556249 , -5.4183836 , -0.28103527, -1.5402354 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"reward\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_history[\"Q\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(episode_history[\"state\"][50],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,11))\n",
    "plt.plot(episode_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"breakout-atari-rl.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = load_model(\"breakout-atari-rl.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "observation, lives = process_obs(observation)\n",
    "\n",
    "episodes = 5\n",
    "done = False\n",
    "epsilon = 0\n",
    "\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    print(\"Welcome to AI Breakout!\")\n",
    "    print(\"Starting Game {} of {}...\".format(i, episodes))\n",
    "    \n",
    "    sum = 0\n",
    "    frame = 1\n",
    "    game = True\n",
    "    observation_batch = []\n",
    "    while game:\n",
    "        \n",
    "        if (frame == 1):\n",
    "            action = 1              \n",
    "        else:\n",
    "            if (frame - 1) % 4 == 0:\n",
    "                action = np.argmax(mod.predict(states)) \n",
    "\n",
    "                if np.array_equal(states[0][:84,:84,2], states[0][:84,:84,3]):\n",
    "                    action = 1\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                observation_batch = []                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation, lives = process_obs(observation)\n",
    "        observation_batch.append(observation)\n",
    "        \n",
    "        if (frame % 4 == 0):\n",
    "            states = np.concatenate([i for i in observation_batch], axis=-1)\n",
    "            states = np.expand_dims(states,0)\n",
    "\n",
    "\n",
    "        sum += reward\n",
    "        frame += 1\n",
    "        time.sleep(0.02)\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            print(\"Total game score was: {}\\n\".format(int(sum)))\n",
    "            observation = env.reset()\n",
    "            observation, lives = process_obs(observation)\n",
    "            game = False\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
